{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7674cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list --export --explicit > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff012aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier #A variant regression for classification tasks!\n",
    "from sklearn.naive_bayes import GaussianNB as NaiveBayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from string import punctuation\n",
    "from scipy.stats import chi2_contingency\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfefe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Loading Dataset\n",
    "data = pd.read_csv('21204829.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf329814",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c5bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c538bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2882b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Null values in each column\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unnamed column as it holds no significance in analysing the data\n",
    "data = data.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39342835",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bafc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 2 Categories of news in our datset\n",
    "# https://www.kaggle.com/code/shivamburnwal/news-articles-classification\n",
    "data.category.value_counts()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/shivamburnwal/news-articles-classification\n",
    "# our unique labels of text which are to be classified.\n",
    "# Check the unique values in the 'category' column\n",
    "unique_categories = data['category'].dropna().unique()\n",
    "print(\"Unique Categories:\")\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8570df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a contingency table to find the p-values of 'category' - 'headline' and 'category' - 'authors'\n",
    "contingency_table1 = pd.crosstab(data['category'], data['headline'])\n",
    "contingency_table2 = pd.crosstab(data['category'], data['authors'])\n",
    "\n",
    "# Performing chi-square test of independence for 'category' and 'headline'\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table1)\n",
    "\n",
    "# Printing the chi-square statistic and p-value\n",
    "print(\"Chi-square statistic for 'category' and 'headline':\", chi2)\n",
    "print(\"p-value for Contingency Table 1:\", p_value)\n",
    "\n",
    "# Performing chi-square test of independence for 'category' and 'author'\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table2)\n",
    "\n",
    "# Printing the chi-square statistic and p-value\n",
    "print(\"Chi-square statistic for 'category' and 'author':\", chi2)\n",
    "print(\"p-value for Contingency Table 2:\", p_value)\n",
    "\n",
    "# Exploring author-specific insights\n",
    "top_authors = data['authors'].value_counts().head(10)\n",
    "\n",
    "for author in top_authors.index:\n",
    "    author_categories = data[data['authors'] == author]['category'].unique()\n",
    "    print(f\"Author: {author}, Categories: {author_categories}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f64243",
   "metadata": {},
   "source": [
    "Based on the analysis, there is no strong evidence to conclude that the category of an article and its headline \n",
    "are associated or dependent on each other. The p-value of 0.4695 indicates that there is a 46.95% probability of \n",
    "observing such a large chi-square statistic under the assumption that the category and headline are independent.\n",
    "\n",
    "The significant result suggests that the category of the article and author are not independent and that changes \n",
    "or patterns in one variable are associated with changes or patterns in the other variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203ce4e",
   "metadata": {},
   "source": [
    "The category and headline variables are compared using the chi-square test of independence to determine whether there is a statistically significant correlation. If the two variables are truly independent, the p-value represents the likelihood of getting the observed outcomes (or more severe ones).\n",
    "We do not have enough data to reject the null hypothesis that there is no correlation between the 'category' and 'headline' variables because the p-value of 0.4760503855701773 is bigger than the commonly accepted significance level of 0.05. In this dataset, the \"category\" and \"headline\" so appear to be independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the most common terms for each category\n",
    "categories = data['category'].unique()\n",
    "for category in categories:\n",
    "    # Filtering the dataframe by category\n",
    "    category_df = data[data['category'] == category]\n",
    "    \n",
    "    # Creating a count vectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    # Fit and transform the preprocessed text\n",
    "    count_matrix = vectorizer.fit_transform(category_df['short_description'])\n",
    "    \n",
    "    # Getting the feature names (terms)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Calculating the term frequencies\n",
    "    term_frequencies = count_matrix.sum(axis=0)\n",
    "    \n",
    "    # Sorting the terms by frequency\n",
    "    sorted_terms = sorted(zip(terms, term_frequencies.tolist()[0]), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Printing the most common terms for the category\n",
    "    print(f\"Most common terms for category before preprocessing: {category}\")\n",
    "    for term, frequency in sorted_terms[:10]:\n",
    "        print(f\"{term}: {frequency}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Converting text to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Removing punctuation\n",
    "        text = ''.join([c for c in text if c not in punctuation])\n",
    "\n",
    "        # Tokenizing text\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Removing stopwords and perform lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
    "\n",
    "        # Joining tokens back to text\n",
    "        text = ' '.join(tokens)\n",
    "    else:\n",
    "        text = \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Applying preprocessing to the 'short_description' column\n",
    "data['preprocessed_text'] = data['short_description'].apply(preprocess_text)\n",
    "\n",
    "# Getting the most common terms for each category\n",
    "categories = data['category'].unique()\n",
    "for category in categories:\n",
    "    # Filtering the dataframe by category\n",
    "    category_df = data[data['category'] == category]\n",
    "    \n",
    "    # Creating a count vectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    # Fit and transform the preprocessed text\n",
    "    count_matrix = vectorizer.fit_transform(category_df['preprocessed_text'])\n",
    "    \n",
    "    # Getting the feature names (terms)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Calculating the term frequencies\n",
    "    term_frequencies = count_matrix.sum(axis=0)\n",
    "    \n",
    "    # Sorting the terms by frequency\n",
    "    sorted_terms = sorted(zip(terms, term_frequencies.tolist()[0]), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Printing the most common terms for the category\n",
    "    print(f\"Most common terms for category after preprocessing: {category}\")\n",
    "    for term, frequency in sorted_terms[:10]:\n",
    "        print(f\"{term}: {frequency}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd08138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 'df' with the actual name of your DataFrame\n",
    "preprocessed_headlines = data['headline'].apply(preprocess_text)\n",
    "\n",
    "# Creating the document-word matrix\n",
    "vectorizer = CountVectorizer(max_features=1000)  # Adjust the max_features parameter as needed\n",
    "document_word_matrix = vectorizer.fit_transform(preprocessed_headlines)\n",
    "\n",
    "# Applying Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 5  # Specify the number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(document_word_matrix)\n",
    "\n",
    "# Getting the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "    top_words_per_topic.append(top_words)\n",
    "\n",
    "# Visualizing the topics using word clouds\n",
    "plt.figure(figsize=(12, 8))\n",
    "for topic_idx, top_words in enumerate(top_words_per_topic):\n",
    "    wordcloud = WordCloud(background_color='white').generate(' '.join(top_words))\n",
    "    plt.subplot(2, 3, topic_idx+1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title('Topic ' + str(topic_idx+1))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'link' and 'date' columns are removed\n",
    "df = data\n",
    "del df['link']\n",
    "del df['date']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentence length for each row\n",
    "data['sentence_length'] = data['preprocessed_text'].str.split().apply(lambda x: len(x))\n",
    "\n",
    "# Grouping the data by category and calculate mean and standard deviation of sentence length\n",
    "category_stats = data.groupby('category')['sentence_length'].agg(['mean', 'std'])\n",
    "\n",
    "# Print the statistics for each category\n",
    "for category, stats in category_stats.iterrows():\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Mean sentence length: {stats['mean']:.2f}\")\n",
    "    print(f\"Standard deviation of sentence length: {stats['std']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c203a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for outliers in sentence length\n",
    "q1 = data['sentence_length'].quantile(0.25)\n",
    "q3 = data['sentence_length'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "outliers = data[(data['sentence_length'] < lower_bound) | (data['sentence_length'] > upper_bound)]\n",
    "print(\"Outliers in Sentence Length:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b413d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplot for each category\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='category', y='sentence_length', data=df)\n",
    "plt.title('Outliers in Sentence Length by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Sentence Length')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data based on sentence length might bias the dataset and affect the analysis of the relationship between \n",
    "# sentence length and the target variable (category).\n",
    "# Data Distribution: Analysing the distribution of sentence lengths across categories to \n",
    "# assess if there are any significant differences\n",
    "# Calculate sentence lengths for each category\n",
    "category_lengths = {}\n",
    "for category in df['category'].unique():\n",
    "    sentences = df[df['category'] == category]['preprocessed_text']\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    category_lengths[category] = sentence_lengths\n",
    "\n",
    "# Plot the distribution of sentence lengths for each category\n",
    "plt.figure(figsize=(10, 6))\n",
    "for category, lengths in category_lengths.items():\n",
    "    plt.hist(lengths, bins=20, alpha=0.5, label=category)\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentence Lengths by Category')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfeba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Balancing\n",
    "# Calculating the initial distribution of categories\n",
    "initial_distribution = df['category'].value_counts()\n",
    "\n",
    "# Removing data based on sentence length\n",
    "max_sentence_length = np.mean(df['sentence_length'])  # Define the maximum sentence length threshold\n",
    "df_filtered = df[df['sentence_length'] <= max_sentence_length]\n",
    "\n",
    "# Calculating the updated distribution of categories\n",
    "updated_distribution = df_filtered['category'].value_counts()\n",
    "\n",
    "# Visualizing the initial and updated distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "initial_distribution.plot(kind='bar', color='blue')\n",
    "plt.title('Initial Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(122)\n",
    "updated_distribution.plot(kind='bar', color='green')\n",
    "plt.title('Updated Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparing the distributions and assess the impact\n",
    "print(\"Initial Distribution:\\n\", initial_distribution)\n",
    "print(\"\\nUpdated Distribution:\\n\", updated_distribution)\n",
    "\n",
    "# Calculating sentence lengths for each category\n",
    "category_lengths = {}\n",
    "for category in df_filtered['category'].unique():\n",
    "    sentences = df_filtered[df_filtered['category'] == category]['preprocessed_text']\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    category_lengths[category] = sentence_lengths\n",
    "\n",
    "# Plotting the distribution of sentence lengths for each category\n",
    "plt.figure(figsize=(10, 6))\n",
    "for category, lengths in category_lengths.items():\n",
    "    plt.hist(lengths, bins=20, alpha=0.5, label=category)\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentence Lengths by Category')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecessary Columns from the data frame 'df_filtered'\n",
    "del df_filtered['short_description']\n",
    "del df_filtered['sentence_length']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning values to X and y\n",
    "X = df_filtered.preprocessed_text\n",
    "y = df_filtered.category\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Splitting the train set further into train and valid sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating DataFrames for the train, valid, and test sets\n",
    "train_data = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "valid_data = pd.DataFrame({'text': X_valid, 'label': y_valid})\n",
    "test_data = pd.DataFrame({'text': X_test, 'label': y_test})\n",
    "\n",
    "# Save the train, valid, and test data to separate CSV files\n",
    "train_data.to_csv('train.csv', index=False)\n",
    "valid_data.to_csv('valid.csv', index=False)\n",
    "test_data.to_csv('test.csv', index=False)\n",
    "\n",
    "# Printing the shapes of the train, valid, and test sets\n",
    "print(f\"Train set shape: {train_data.shape}\")\n",
    "print(f\"Valid set shape: {valid_data.shape}\")\n",
    "print(f\"Test set shape: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if each columns have null values\n",
    "df_filtered.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with empty strings\n",
    "train_data['text'] = train_data['text'].fillna('')\n",
    "valid_data['text'] = valid_data['text'].fillna('')\n",
    "\n",
    "# Converting text into a numeric representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_valid = vectorizer.transform(valid_data['text'])\n",
    "\n",
    "# Getting the labels\n",
    "y_train = train_data['label']\n",
    "y_valid = valid_data['label']\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the models\n",
    "models = [('Logistic Regression', LogisticRegression(max_iter=500)),\n",
    "          ('Random Forest', RandomForestClassifier()),\n",
    "          ('Linear SVC', LinearSVC()),\n",
    "          ('Multinomial NaiveBayes', MultinomialNB()),\n",
    "          ('SGD Classifier', SGDClassifier())]\n",
    "\n",
    "names = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over each model\n",
    "for name, clf in models:\n",
    "    # Create a pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('clf', clf),\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_pred = pipe.predict(X_valid)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred, pos_label='QUEER VOICES')\n",
    "    recall = recall_score(y_valid, y_pred, pos_label='QUEER VOICES')\n",
    "    f1 = f1_score(y_valid, y_pred, pos_label='QUEER VOICES')  # Calculate the F1 score\n",
    "    \n",
    "    # Store the model and scores\n",
    "    names.append(name)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Save the model\n",
    "    filename = f'{name}.joblib'\n",
    "    dump(pipe, filename)\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    plot_confusion_matrix(pipe, X_valid, y_valid, cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "scores_df = pd.DataFrame({\n",
    "    'Model': names,\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(scores_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.learndatasci.com/glossary/binary-classification/\n",
    "# Plot showing Accuracy, Precision and Recall scores of all the 5 binary classifiers\n",
    "ax = scores_df.plot.barh(x='Model', y=['Accuracy', 'Precision', 'Recall'], stacked=True)\n",
    "ax.legend(ncol=len(models), bbox_to_anchor=(0, 1), loc='lower left', prop={'size': 14})\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Model')\n",
    "plt.title('Model Performance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised CNN Model and evaluating the accuracy and F1-score\n",
    "# Loading the train and validation datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "valid_data = pd.read_csv(\"valid.csv\")\n",
    "\n",
    "# Dropping rows with missing values\n",
    "train_data = train_data.dropna()\n",
    "valid_data = valid_data.dropna()\n",
    "\n",
    "# Extracting the text and label columns\n",
    "train_text = train_data[\"text\"].tolist()\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "valid_text = valid_data[\"text\"].tolist()\n",
    "valid_labels = valid_data[\"label\"].tolist()\n",
    "\n",
    "# Encoding labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "valid_labels = label_encoder.transform(valid_labels)\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_text)\n",
    "\n",
    "# Padding sequences to have the same length\n",
    "max_sequence_length = max(len(seq) for seq in train_sequences)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "valid_sequences = pad_sequences(valid_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Creating the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(train_sequences, train_labels, batch_size=32, epochs=10, validation_data=(valid_sequences, valid_labels))\n",
    "\n",
    "# Saving the model\n",
    "model.save(\"cnn_model.h5\")\n",
    "\n",
    "# Evaluating the model\n",
    "valid_pred = model.predict(valid_sequences)\n",
    "valid_pred_labels = np.argmax(valid_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(valid_labels, valid_pred_labels)\n",
    "precision = precision_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "recall = recall_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "f1 = f1_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "\n",
    "# Printing the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the saved CNN model to evaluate its performance on train and valid datasets\n",
    "\n",
    "# Loading the saved model\n",
    "loaded_model = load_model(\"cnn_model.h5\")\n",
    "\n",
    "# Evaluating the loaded model on train data\n",
    "train_pred = loaded_model.predict(train_sequences)\n",
    "train_pred_labels = np.argmax(train_pred, axis=1)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_pred_labels)\n",
    "train_precision = precision_score(train_labels, train_pred_labels, average='weighted')\n",
    "train_recall = recall_score(train_labels, train_pred_labels, average='weighted')\n",
    "train_f1 = f1_score(train_labels, train_pred_labels, average='weighted')\n",
    "\n",
    "# Evaluating the loaded model on valid data\n",
    "valid_pred = loaded_model.predict(valid_sequences)\n",
    "valid_pred_labels = np.argmax(valid_pred, axis=1)\n",
    "\n",
    "valid_accuracy = accuracy_score(valid_labels, valid_pred_labels)\n",
    "valid_precision = precision_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "valid_recall = recall_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "valid_f1 = f1_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "\n",
    "# Printing the evaluation metrics for train and valid data\n",
    "print(\"Train Data Metrics:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "print()\n",
    "print(\"Valid Data Metrics:\")\n",
    "print(f\"Accuracy: {valid_accuracy:.4f}\")\n",
    "print(f\"Precision: {valid_precision:.4f}\")\n",
    "print(f\"Recall: {valid_recall:.4f}\")\n",
    "print(f\"F1 Score: {valid_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the binary classifiers for accuracy and F1-scores on train and valitaion data\n",
    "# Fit the vectorizer on the entire train data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Transform the train and validation text data using the vectorizer\n",
    "valid_vectors = vectorizer.transform(valid_data['text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix\n",
    "valid_vectors = valid_vectors.toarray()\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Check for any extra features in the validation data\n",
    "extra_features = set(feature_names) - set(vectorizer.get_feature_names())\n",
    "\n",
    "if extra_features:\n",
    "    # Filter out the extra features from the validation vectors\n",
    "    valid_vectors = valid_vectors[:, [i for i, feature_name in enumerate(feature_names) if feature_name not in extra_features]]\n",
    "\n",
    "# Perform padding on the input vectors\n",
    "max_sequence_length = train_vectors.shape[1]\n",
    "valid_vectors = pad_sequences(valid_vectors, maxlen=max_sequence_length)    \n",
    "# train_vectors = pad_sequences(train_vectors, maxlen=max_sequence_length)    \n",
    "\n",
    "\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'Linear SVC', 'Multinomial NaiveBayes', 'SGD Classifier']\n",
    "file_names = ['Logistic Regression.joblib', 'Random Forest.joblib', 'Linear SVC.joblib', 'Multinomial NaiveBayes.joblib', 'SGD Classifier.joblib']\n",
    "    \n",
    "# Load and evaluate each saved model\n",
    "for model_name, file_name in zip(model_names, file_names):\n",
    "    # Load the model\n",
    "    if file_name.endswith('.joblib'):\n",
    "        model = load(file_name)\n",
    "    elif file_name.endswith('.h5'):\n",
    "        model = load_model(file_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_name}\")\n",
    "\n",
    "    # Make predictions on the train and validation datasets\n",
    "    train_pred = model.predict(train_vectors)\n",
    "    valid_pred = model.predict(valid_vectors)\n",
    "\n",
    "    # Calculate evaluation metrics for the train dataset\n",
    "    accuracy_train = accuracy_score(train_data['label'], train_pred)\n",
    "    precision_train = precision_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "    recall_train = recall_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "    f1_train = f1_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "\n",
    "    # Calculate evaluation metrics for the validation dataset\n",
    "    accuracy_valid = accuracy_score(valid_data['label'], valid_pred)\n",
    "    precision_valid = precision_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "    recall_valid = recall_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "    f1_valid = f1_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "\n",
    "    # Print the evaluation metrics for the train and validation datasets\n",
    "    print(f\"Train Metrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "    print(f\"Precision: {precision_train:.4f}\")\n",
    "    print(f\"Recall: {recall_train:.4f}\")\n",
    "    print(f\"F1 Score: {f1_train:.4f}\")\n",
    "    print()\n",
    "    print(f\"Validation Metrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy_valid:.4f}\")\n",
    "    print(f\"Precision: {precision_valid:.4f}\")\n",
    "    print(f\"Recall: {recall_valid:.4f}\")\n",
    "    print(f\"F1 Score: {f1_valid:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved CNN model and evaluating for its performance (acuuracy and F1-score) on train and validation data\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"cnn_model.h5\")\n",
    "\n",
    "# Evaluate the loaded model on train data\n",
    "train_pred = loaded_model.predict(train_sequences)\n",
    "train_pred_labels = np.argmax(train_pred, axis=1)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_pred_labels)\n",
    "train_precision = precision_score(train_labels, train_pred_labels, average='weighted')\n",
    "train_recall = recall_score(train_labels, train_pred_labels, average='weighted')\n",
    "train_f1 = f1_score(train_labels, train_pred_labels, average='weighted')\n",
    "\n",
    "# Evaluate the loaded model on valid data\n",
    "valid_pred = loaded_model.predict(valid_sequences)\n",
    "valid_pred_labels = np.argmax(valid_pred, axis=1)\n",
    "\n",
    "valid_accuracy = accuracy_score(valid_labels, valid_pred_labels)\n",
    "valid_precision = precision_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "valid_recall = recall_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "valid_f1 = f1_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics for train and valid data\n",
    "print(\"Train Data Metrics for CNN:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "print()\n",
    "print(\"Valid Data Metrics for CNN:\")\n",
    "print(f\"Accuracy: {valid_accuracy:.4f}\")\n",
    "print(f\"Precision: {valid_precision:.4f}\")\n",
    "print(f\"Recall: {valid_recall:.4f}\")\n",
    "print(f\"F1 Score: {valid_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfedd306",
   "metadata": {},
   "source": [
    "\n",
    "Looking at the performance metrics on the train and validation sets, we can observe the following:\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "Train Accuracy: 0.9576, Validation Accuracy: 0.6381\n",
    "Train F1 Score: 0.9694, Validation F1 Score: 0.7751\n",
    "The accuracy, precision, recall, and F1 score on the train set are higher compared to the validation set. This indicates that the model is slightly overfitting the train data, as the performance drops slightly on the validation set. However, the drop in performance is relatively small, suggesting that the model generalizes well.\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "Train Accuracy: 0.7989, Validation Accuracy: 0.6349\n",
    "Train F1 Score: 0.8699, Validation F1 Score: 0.7754\n",
    "Similar to Logistic Regression, the Random Forest model shows higher accuracy, precision, recall, and F1 score on the train set compared to the validation set. This indicates a slight overfitting of the train data, but the drop in performance on the validation set is not significant.\n",
    "\n",
    "Linear SVC:\n",
    "\n",
    "Train Accuracy: 0.9939, Validation Accuracy: 0.6397\n",
    "Train F1 Score: 0.9955, Validation F1 Score: 0.7754\n",
    "The Linear SVC model demonstrates very high performance on both the train and validation sets. The metrics on the train set are nearly identical to those on the validation set, indicating that the model generalizes well and does not suffer from overfitting. It achieves high accuracy, precision, recall, and F1 score on both datasets.\n",
    "\n",
    "Multinomial NaiveBayes:\n",
    "\n",
    "Train Accuracy: 0.9540, Validation Accuracy: 0.6365\n",
    "Train F1 Score: 0.9666, Validation F1 Score: 0.7739\n",
    "Similar to the previous models, Multinomial NaiveBayes exhibits slightly higher performance on the train set compared to the validation set. The drop in performance on the validation set is relatively small, indicating reasonable generalization capabilities. The model achieves good accuracy, precision, recall, and F1 score on both datasets.\n",
    "\n",
    "SGD Classifier:\n",
    "\n",
    "Train Accuracy: 0.9851, Validation Accuracy: 0.6414\n",
    "Train F1 Score: 0.9890, Validation F1 Score: 0.7767\n",
    "The SGD Classifier model performs exceptionally well on both the train and validation sets. It achieves high accuracy, precision, recall, and F1 score on both datasets. The metrics on the train set are almost identical to those on the validation set, suggesting excellent generalization.\n",
    "\n",
    "CNN:\n",
    "\n",
    "Train Accuracy: 0.9988, Validation Accuracy: 0.7964\n",
    "Train F1 Score: 0.9988, Validation F1 Score: 0.7944\n",
    "The CNN model exhibits a slight decrease in performance when transitioning from the train set to the validation set. It achieves a train accuracy of 0.9988 and a validation accuracy of 0.7964, as well as a train F1 score of 0.9988 and a validation F1 score of 0.7944. Although the model maintains a high level of accuracy and F1 score on the train set, it encounters difficulties in generalizing to unseen data, resulting in slightly lower performance on the validation set. This suggests that the model might be overfitting the train data, emphasizing specific patterns and noise that are not representative of the overall dataset. Adjustments such as regularization techniques or fine-tuning the model architecture could help improve its ability to generalize and enhance performance on unseen data.\n",
    "\n",
    "In summary, the Logistic Regression, Linear SVC, Multinomial NaiveBayes, SGD Classifier models perform well and show good generalization, with similar performance on both the training and validation sets. Random Forest also performs reasonably well but exhibits a slight drop in performance on the validation set. However, the CNN model exhibits a slight decrease in performance when transitioning from the train set to the validation set. It maintains high accuracy and F1 score on the train set but encounters difficulties in generalizing to unseen data, resulting in slightly lower performance on the validation set. This indicates a potential overfitting of the train data and suggests the need for adjustments to improve generalization.\n",
    "\n",
    "Overall, the Linear SVC and SGD Classifier models showcase the best generalization capabilities, while the CNN model requires further fine-tuning to enhance its ability to generalize and perform well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf815234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis of the binary classifiers and finding the misclassified sentences in train and valid data\n",
    "# Fit the vectorizer on the entire train data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Transform the train and validation text data using the vectorizer\n",
    "valid_vectors = vectorizer.transform(valid_data['text'])\n",
    "\n",
    "# Converting the sparse matrix to a dense matrix\n",
    "valid_vectors = valid_vectors.toarray()\n",
    "\n",
    "# Getting the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Checking for any extra features in the validation data\n",
    "extra_features = set(feature_names) - set(vectorizer.get_feature_names())\n",
    "\n",
    "if extra_features:\n",
    "    # Filtering out the extra features from the validation vectors\n",
    "    valid_vectors = valid_vectors[:, [i for i, feature_name in enumerate(feature_names) if feature_name not in extra_features]]\n",
    "\n",
    "# Performing padding on the input vectors\n",
    "max_sequence_length = train_vectors.shape[1]\n",
    "valid_vectors = pad_sequences(valid_vectors, maxlen=max_sequence_length)\n",
    "\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'Linear SVC', 'Multinomial NaiveBayes', 'SGD Classifier']\n",
    "file_names = ['Logistic Regression.joblib', 'Random Forest.joblib', 'Linear SVC.joblib', 'Multinomial NaiveBayes.joblib', 'SGD Classifier.joblib']\n",
    "\n",
    "# Loading and evaluating each saved model\n",
    "for model_name, file_name in zip(model_names, file_names):\n",
    "    # Load the model\n",
    "    if file_name.endswith('.joblib'):\n",
    "        model = load(file_name)\n",
    "    elif file_name.endswith('.h5'):\n",
    "        model = load_model(file_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_name}\")\n",
    "\n",
    "    # Making predictions on the train and validation datasets\n",
    "    train_pred = model.predict(train_vectors)\n",
    "    valid_pred = model.predict(valid_vectors)\n",
    "\n",
    "    # Performing error analysis\n",
    "    train_errors = train_data[train_data['label'] != train_pred]\n",
    "    valid_errors = valid_data[valid_data['label'] != valid_pred]\n",
    "\n",
    "    print(f\"Error Analysis for {model_name}:\")\n",
    "    print(\"Train Errors:\")\n",
    "    print(train_errors)\n",
    "    print()\n",
    "\n",
    "    print(\"Validation Errors:\")\n",
    "    print(valid_errors)\n",
    "    print()\n",
    "\n",
    "    print(\"-------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe43bc",
   "metadata": {},
   "source": [
    "The error analysis conducted on Linear Regression, Random Forest, Linear SVC, Naive Bayes and SGD Classifier models reveals the misclassified examples for each model and provides insights into the patterns of misclassification. Let's examine if the different models classified the same sentences incorrectly and summarize the findings:\n",
    "\n",
    "From the error analysis, it appears that the different models classified some sentences incorrectly in common. Here are the observations:\n",
    "\n",
    "The sentences misclassified by multiple models are primarily from the \"TASTE\" label. Some examples of these common misclassifications include sentences like \"get ready,\" \"weirdest debate 2017,\" and \"okay lady let's get formation.\" These sentences seem to have ambiguous or context-dependent meanings, making them challenging to classify correctly.\n",
    "\n",
    "The misclassifications are more prevalent in the \"TASTE\" label, indicating that the models struggle to distinguish between different topics related to food, recipes, and flavors.\n",
    "\n",
    "The models tend to misclassify shorter or less informative sentences, as seen in examples like \"bottom,\" \"huge,\" or \"look.\" These sentences lack specific context or clear indicators for classification, leading to errors.\n",
    "\n",
    "It is interesting to note that the misclassified examples have varying degrees of severity. Some sentences could be considered subjective or open to interpretation, making it challenging even for human annotators to assign a definitive label.\n",
    "\n",
    "Overall, the analysis suggests that the models face difficulty in accurately classifying certain sentences related to food, flavors, and subjective topics. Improving the models' performance in distinguishing such nuanced and context-dependent sentences would require additional training data and more fine-tuning of the models' parameters.\n",
    "\n",
    "It is important to continue analyzing and refining the models to address these common misclassifications and enhance their overall accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis of the CNN Model and finding the misclassified sentences in train and valid data\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"cnn_model.h5\")\n",
    "\n",
    "# Get the predicted labels for train and valid datasets\n",
    "train_pred_labels = np.argmax(loaded_model.predict(train_sequences), axis=1)\n",
    "valid_pred_labels = np.argmax(loaded_model.predict(valid_sequences), axis=1)\n",
    "\n",
    "# Get the original labels for train and valid datasets\n",
    "train_original_labels = train_labels\n",
    "valid_original_labels = valid_labels\n",
    "\n",
    "# Create a DataFrame to store the results for train dataset\n",
    "train_error_analysis_df = pd.DataFrame({\"Text\": train_text, \"Original Label\": train_original_labels, \"Predicted Label\": train_pred_labels})\n",
    "\n",
    "# Create a DataFrame to store the results for valid dataset\n",
    "valid_error_analysis_df = pd.DataFrame({\"Text\": valid_text, \"Original Label\": valid_original_labels, \"Predicted Label\": valid_pred_labels})\n",
    "\n",
    "# Filter the DataFrames to get misclassified examples\n",
    "train_misclassified_df = train_error_analysis_df[train_error_analysis_df[\"Original Label\"] != train_error_analysis_df[\"Predicted Label\"]]\n",
    "valid_misclassified_df = valid_error_analysis_df[valid_error_analysis_df[\"Original Label\"] != valid_error_analysis_df[\"Predicted Label\"]]\n",
    "\n",
    "# Print the misclassified examples for train dataset\n",
    "print(\"Misclassified Examples in Train Dataset:\")\n",
    "print(train_misclassified_df)\n",
    "\n",
    "# Print the misclassified examples for valid dataset\n",
    "print(\"Misclassified Examples in Valid Dataset:\")\n",
    "print(valid_misclassified_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6b1ec",
   "metadata": {},
   "source": [
    "From the error analysis for the CNN model, it is evident that the misclassifications are primarily occurring in the \"QUEER VOICES\" label. However, it is important to note that the CNN model has misclassified a significant number of sentences compared to the other models. Here are the observations:\n",
    "\n",
    "The CNN model misclassifies a large number of sentences from the \"QUEER VOICES\" label in both the train and validation sets. These sentences contain various topics related to LGBTQ+ issues, such as discussions on political figures, personal experiences, societal challenges, and the LGBTQ+ community.\n",
    "\n",
    "Unlike the other models, the CNN model does not misclassify any sentences as \"TASTE\" in both the train and validation sets. This indicates that the CNN model's misclassifications are specific to the \"QUEER VOICES\" label and not related to food or flavors.\n",
    "\n",
    "The confusion matrix for the CNN model shows that it misclassifies all the examples in the \"QUEER VOICES\" label, while correctly classifying all the examples in the \"TASTE\" label. This indicates a significant bias in the model's predictions towards the \"TASTE\" label.\n",
    "\n",
    "In comparison to the other models, the CNN model performs poorly in terms of misclassifications, especially for the \"QUEER VOICES\" label. This suggests that the CNN model is not effectively capturing the patterns and nuances of LGBTQ+ content in the dataset. It may require further training, fine-tuning, or architectural adjustments to improve its performance and reduce bias.\n",
    "\n",
    "Overall, the analysis highlights the challenges faced by the CNN model in accurately classifying sentences related to LGBTQ+ topics. Addressing these misclassifications would be crucial for improving the model's ability to capture the nuances and complexities of the \"QUEER VOICES\" label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a785fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making changes to the Binary Classifiers by changing its parameters and saving the updated models\n",
    "\n",
    "# Fit the vectorizer on the entire train data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Transform the train and validation text data using the vectorizer\n",
    "valid_vectors = vectorizer.transform(valid_data['text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix\n",
    "valid_vectors = valid_vectors.toarray()\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Check for any extra features in the validation data\n",
    "extra_features = set(feature_names) - set(vectorizer.get_feature_names())\n",
    "\n",
    "if extra_features:\n",
    "    # Filter out the extra features from the validation vectors\n",
    "    valid_vectors = valid_vectors[:, [i for i, feature_name in enumerate(feature_names) if feature_name not in extra_features]]\n",
    "\n",
    "# Perform padding on the input vectors\n",
    "max_sequence_length = train_vectors.shape[1]\n",
    "valid_vectors = pad_sequences(valid_vectors, maxlen=max_sequence_length)\n",
    "\n",
    "# Apply changes to each classifier/model and re-evaluate\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_model = LogisticRegression(class_weight='balanced')\n",
    "logreg_model.fit(train_vectors, train_data['label'])\n",
    "logreg_pred_train = logreg_model.predict(train_vectors)\n",
    "logreg_pred_valid = logreg_model.predict(valid_vectors)\n",
    "logreg_accuracy_train = accuracy_score(train_data['label'], logreg_pred_train)\n",
    "logreg_accuracy_valid = accuracy_score(valid_data['label'], logreg_pred_valid)\n",
    "logreg_precision_train = precision_score(train_data['label'], logreg_pred_train, pos_label='QUEER VOICES')\n",
    "logreg_precision_valid = precision_score(valid_data['label'], logreg_pred_valid, pos_label='QUEER VOICES')\n",
    "logreg_recall_train = recall_score(train_data['label'], logreg_pred_train, pos_label='QUEER VOICES')\n",
    "logreg_recall_valid = recall_score(valid_data['label'], logreg_pred_valid, pos_label='QUEER VOICES')\n",
    "logreg_f1_train = f1_score(train_data['label'], logreg_pred_train, pos_label='QUEER VOICES')\n",
    "logreg_f1_valid = f1_score(valid_data['label'], logreg_pred_valid, pos_label='QUEER VOICES')\n",
    "\n",
    "# Random Forest\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100)\n",
    "random_forest_model.fit(train_vectors, train_data['label'])\n",
    "random_forest_pred_train = random_forest_model.predict(train_vectors)\n",
    "random_forest_pred_valid = random_forest_model.predict(valid_vectors)\n",
    "random_forest_accuracy_train = accuracy_score(train_data['label'], random_forest_pred_train)\n",
    "random_forest_accuracy_valid = accuracy_score(valid_data['label'], random_forest_pred_valid)\n",
    "random_forest_precision_train = precision_score(train_data['label'], random_forest_pred_train, pos_label='QUEER VOICES')\n",
    "random_forest_precision_valid = precision_score(valid_data['label'], random_forest_pred_valid, pos_label='QUEER VOICES')\n",
    "random_forest_recall_train = recall_score(train_data['label'], random_forest_pred_train, pos_label='QUEER VOICES')\n",
    "random_forest_recall_valid = recall_score(valid_data['label'], random_forest_pred_valid, pos_label='QUEER VOICES')\n",
    "random_forest_f1_train = f1_score(train_data['label'], random_forest_pred_train, pos_label='QUEER VOICES')\n",
    "random_forest_f1_valid = f1_score(valid_data['label'], random_forest_pred_valid, pos_label='QUEER VOICES')\n",
    "\n",
    "# Linear SVC\n",
    "linear_svc_model = LinearSVC(class_weight='balanced')\n",
    "linear_svc_model.fit(train_vectors, train_data['label'])\n",
    "linear_svc_pred_train = linear_svc_model.predict(train_vectors)\n",
    "linear_svc_pred_valid = linear_svc_model.predict(valid_vectors)\n",
    "linear_svc_accuracy_train = accuracy_score(train_data['label'], linear_svc_pred_train)\n",
    "linear_svc_accuracy_valid = accuracy_score(valid_data['label'], linear_svc_pred_valid)\n",
    "linear_svc_precision_train = precision_score(train_data['label'], linear_svc_pred_train, pos_label='QUEER VOICES')\n",
    "linear_svc_precision_valid = precision_score(valid_data['label'], linear_svc_pred_valid, pos_label='QUEER VOICES')\n",
    "linear_svc_recall_train = recall_score(train_data['label'], linear_svc_pred_train, pos_label='QUEER VOICES')\n",
    "linear_svc_recall_valid = recall_score(valid_data['label'], linear_svc_pred_valid, pos_label='QUEER VOICES')\n",
    "linear_svc_f1_train = f1_score(train_data['label'], linear_svc_pred_train, pos_label='QUEER VOICES')\n",
    "linear_svc_f1_valid = f1_score(valid_data['label'], linear_svc_pred_valid, pos_label='QUEER VOICES')\n",
    "\n",
    "# SGD Classifier\n",
    "sgd_classifier_model = SGDClassifier(class_weight='balanced')\n",
    "sgd_classifier_model.fit(train_vectors, train_data['label'])\n",
    "sgd_classifier_pred_train = sgd_classifier_model.predict(train_vectors)\n",
    "sgd_classifier_pred_valid = sgd_classifier_model.predict(valid_vectors)\n",
    "sgd_classifier_accuracy_train = accuracy_score(train_data['label'], sgd_classifier_pred_train)\n",
    "sgd_classifier_accuracy_valid = accuracy_score(valid_data['label'], sgd_classifier_pred_valid)\n",
    "sgd_classifier_precision_train = precision_score(train_data['label'], sgd_classifier_pred_train, pos_label='QUEER VOICES')\n",
    "sgd_classifier_precision_valid = precision_score(valid_data['label'], sgd_classifier_pred_valid, pos_label='QUEER VOICES')\n",
    "sgd_classifier_recall_train = recall_score(train_data['label'], sgd_classifier_pred_train, pos_label='QUEER VOICES')\n",
    "sgd_classifier_recall_valid = recall_score(valid_data['label'], sgd_classifier_pred_valid, pos_label='QUEER VOICES')\n",
    "sgd_classifier_f1_train = f1_score(train_data['label'], sgd_classifier_pred_train, pos_label='QUEER VOICES')\n",
    "sgd_classifier_f1_valid = f1_score(valid_data['label'], sgd_classifier_pred_valid, pos_label='QUEER VOICES')\n",
    "\n",
    "# Save the updated models\n",
    "dump(logreg_model, 'LogisticRegression_updated.joblib')\n",
    "dump(random_forest_model, 'RandomForest_updated.joblib')\n",
    "dump(linear_svc_model, 'LinearSVC_updated.joblib')\n",
    "dump(sgd_classifier_model, 'SGDClassifier_updated.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation of all the saved updated binary classifiers on train and valid datasets\n",
    "\n",
    "# Fit the vectorizer on the entire train data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Transform the train and validation text data using the vectorizer\n",
    "valid_vectors = vectorizer.transform(valid_data['text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix\n",
    "valid_vectors = valid_vectors.toarray()\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Check for any extra features in the validation data\n",
    "extra_features = set(feature_names) - set(vectorizer.get_feature_names())\n",
    "\n",
    "if extra_features:\n",
    "    # Filter out the extra features from the validation vectors\n",
    "    valid_vectors = valid_vectors[:, [i for i, feature_name in enumerate(feature_names) if feature_name not in extra_features]]\n",
    "\n",
    "# Perform padding on the input vectors\n",
    "max_sequence_length = train_vectors.shape[1]\n",
    "valid_vectors = pad_sequences(valid_vectors, maxlen=max_sequence_length)\n",
    "\n",
    "# List of model names and corresponding file names\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'Linear SVC', 'SGD Classifier']\n",
    "file_names = ['LogisticRegression_updated.joblib', 'RandomForest_updated.joblib', 'LinearSVC_updated.joblib', 'SGDClassifier_updated.joblib']\n",
    "\n",
    "# Load and evaluate each saved model\n",
    "for model_name, file_name in zip(model_names, file_names):\n",
    "    # Load the model\n",
    "    model = load(file_name)\n",
    "\n",
    "    # Make predictions on the train and validation datasets\n",
    "    train_pred = model.predict(train_vectors)\n",
    "    valid_pred = model.predict(valid_vectors)\n",
    "\n",
    "    # Calculate evaluation metrics for the train dataset\n",
    "    accuracy_train = accuracy_score(train_data['label'], train_pred)\n",
    "    precision_train = precision_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "    recall_train = recall_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "    f1_train = f1_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "\n",
    "    # Calculate evaluation metrics for the validation dataset\n",
    "    accuracy_valid = accuracy_score(valid_data['label'], valid_pred)\n",
    "    precision_valid = precision_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "    recall_valid = recall_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "    f1_valid = f1_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "\n",
    "    # Print the evaluation metrics for both the training and validation datasets\n",
    "    print(f\"Metrics for {model_name}:\")\n",
    "    print(f\"Train - Accuracy: {accuracy_train:.4f}, Precision: {precision_train:.4f}, Recall: {recall_train:.4f}, F1 Score: {f1_train:.4f}\")\n",
    "    print(f\"Validation - Accuracy: {accuracy_valid:.4f}, Precision: {precision_valid:.4f}, Recall: {recall_valid:.4f}, F1 Score: {f1_valid:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation of the updated and saved Linear SVC and SGD Classifier models\n",
    "# The performance of the two models are comparitively better than other binary classifiers\n",
    "\n",
    "# Fit the vectorizer on the entire train data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Transform the train and validation text data using the vectorizer\n",
    "valid_vectors = vectorizer.transform(valid_data['text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix\n",
    "valid_vectors = valid_vectors.toarray()\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Check for any extra features in the validation data\n",
    "extra_features = set(feature_names) - set(vectorizer.get_feature_names())\n",
    "\n",
    "if extra_features:\n",
    "    # Filter out the extra features from the validation vectors\n",
    "    valid_vectors = valid_vectors[:, [i for i, feature_name in enumerate(feature_names) if feature_name not in extra_features]]\n",
    "\n",
    "# Perform padding on the input vectors\n",
    "max_sequence_length = train_vectors.shape[1]\n",
    "valid_vectors = pad_sequences(valid_vectors, maxlen=max_sequence_length)    \n",
    "# train_vectors = pad_sequences(train_vectors, maxlen=max_sequence_length)    \n",
    "\n",
    "\n",
    "model_names = ['Updated Linear SVC', 'Updated SGD Classifier']\n",
    "file_names = ['LinearSVC_updated.joblib', 'SGDClassifier_updated.joblib']\n",
    "    \n",
    "# Load and evaluate each saved model\n",
    "for model_name, file_name in zip(model_names, file_names):\n",
    "    # Load the model\n",
    "    if file_name.endswith('.joblib'):\n",
    "        model = load(file_name)\n",
    "    elif file_name.endswith('.h5'):\n",
    "        model = load_model(file_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_name}\")\n",
    "\n",
    "    # Make predictions on the train and validation datasets\n",
    "    train_pred = model.predict(train_vectors)\n",
    "    valid_pred = model.predict(valid_vectors)\n",
    "\n",
    "    # Calculate evaluation metrics for the train dataset\n",
    "    accuracy_train = accuracy_score(train_data['label'], train_pred)\n",
    "    precision_train = precision_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "    recall_train = recall_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "    f1_train = f1_score(train_data['label'], train_pred, pos_label='QUEER VOICES')\n",
    "\n",
    "    # Calculate evaluation metrics for the validation dataset\n",
    "    accuracy_valid = accuracy_score(valid_data['label'], valid_pred)\n",
    "    precision_valid = precision_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "    recall_valid = recall_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "    f1_valid = f1_score(valid_data['label'], valid_pred, pos_label='QUEER VOICES')\n",
    "\n",
    "    # Print the evaluation metrics for the train and validation datasets\n",
    "    print(f\"Train Metrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "    print(f\"Precision: {precision_train:.4f}\")\n",
    "    print(f\"Recall: {recall_train:.4f}\")\n",
    "    print(f\"F1 Score: {f1_train:.4f}\")\n",
    "    print()\n",
    "    print(f\"Validation Metrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy_valid:.4f}\")\n",
    "    print(f\"Precision: {precision_valid:.4f}\")\n",
    "    print(f\"Recall: {recall_valid:.4f}\")\n",
    "    print(f\"F1 Score: {f1_valid:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404123fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updateing the CNN model by making changes to its parameters and saving the updated model\n",
    "# Load the train and validation datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "valid_data = pd.read_csv(\"valid.csv\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_data = train_data.dropna()\n",
    "valid_data = valid_data.dropna()\n",
    "\n",
    "# Extract the text and label columns\n",
    "train_text = train_data[\"text\"].tolist()\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "valid_text = valid_data[\"text\"].tolist()\n",
    "valid_labels = valid_data[\"label\"].tolist()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "valid_labels = label_encoder.transform(valid_labels)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_text)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_sequence_length = max(len(seq) for seq in train_sequences)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "valid_sequences = pad_sequences(valid_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with modified parameters\n",
    "model.fit(train_sequences, train_labels, batch_size=64, epochs=20, validation_data=(valid_sequences, valid_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "valid_pred = model.predict(valid_sequences)\n",
    "valid_pred_labels = np.argmax(valid_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(valid_labels, valid_pred_labels)\n",
    "precision = precision_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "recall = recall_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "f1 = f1_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy for CNN model: {accuracy:.4f}\")\n",
    "print(f\"Precision score for CNN model: {precision:.4f}\")\n",
    "print(f\"Recall score for CNN model: {recall:.4f}\")\n",
    "print(f\"F1 Score for CNN model: {f1:.4f}\")\n",
    "\n",
    "# Save the updated CNN model\n",
    "model.save('cnn_model_updated.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1445d2",
   "metadata": {},
   "source": [
    "Both Linear SVC Model, SGD Classifier and Random Forest have achieved the desired accuracy of 99% and F1 score of 0.99.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation of the updated CNN model\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"cnn_model_updated.h5\")\n",
    "\n",
    "# Evaluate the loaded model on train data\n",
    "train_pred = loaded_model.predict(train_sequences)\n",
    "train_pred_labels = np.argmax(train_pred, axis=1)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_pred_labels)\n",
    "train_precision = precision_score(train_labels, train_pred_labels, average='weighted')\n",
    "train_recall = recall_score(train_labels, train_pred_labels, average='weighted')\n",
    "train_f1 = f1_score(train_labels, train_pred_labels, average='weighted')\n",
    "\n",
    "# Evaluate the loaded model on valid data\n",
    "valid_pred = loaded_model.predict(valid_sequences)\n",
    "valid_pred_labels = np.argmax(valid_pred, axis=1)\n",
    "\n",
    "valid_accuracy = accuracy_score(valid_labels, valid_pred_labels)\n",
    "valid_precision = precision_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "valid_recall = recall_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "valid_f1 = f1_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics for train and valid data\n",
    "print(\"Train Data Metrics for Updated CNN:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "print()\n",
    "print(\"Valid Data Metrics for Updated CNN:\")\n",
    "print(f\"Accuracy: {valid_accuracy:.4f}\")\n",
    "print(f\"Precision: {valid_precision:.4f}\")\n",
    "print(f\"Recall: {valid_recall:.4f}\")\n",
    "print(f\"F1 Score: {valid_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis of the updated Linear SVC and SGD Classifier model on train and valid dataset\n",
    "# Fit the vectorizer on the entire train data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Transform the train and validation text data using the vectorizer\n",
    "valid_vectors = vectorizer.transform(valid_data['text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix\n",
    "valid_vectors = valid_vectors.toarray()\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Check for any extra features in the validation data\n",
    "extra_features = set(feature_names) - set(vectorizer.get_feature_names())\n",
    "\n",
    "if extra_features:\n",
    "    # Filter out the extra features from the validation vectors\n",
    "    valid_vectors = valid_vectors[:, [i for i, feature_name in enumerate(feature_names) if feature_name not in extra_features]]\n",
    "\n",
    "# Perform padding on the input vectors\n",
    "max_sequence_length = train_vectors.shape[1]\n",
    "valid_vectors = pad_sequences(valid_vectors, maxlen=max_sequence_length)\n",
    "\n",
    "model_names = ['Linear SVC Updated','SGD Classifier Updated']\n",
    "file_names = ['LinearSVC_updated.joblib','SGDClassifier_updated.joblib']\n",
    "\n",
    "# Load and evaluate each saved model\n",
    "for model_name, file_name in zip(model_names, file_names):\n",
    "    # Load the model\n",
    "    if file_name.endswith('.joblib'):\n",
    "        model = load(file_name)\n",
    "    elif file_name.endswith('.h5'):\n",
    "        model = load_model(file_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_name}\")\n",
    "\n",
    "    # Make predictions on the train and validation datasets\n",
    "    train_pred = model.predict(train_vectors)\n",
    "    valid_pred = model.predict(valid_vectors)\n",
    "\n",
    "    # Perform error analysis\n",
    "    train_errors = train_data[train_data['label'] != train_pred]\n",
    "    valid_errors = valid_data[valid_data['label'] != valid_pred]\n",
    "\n",
    "    print(f\"Error Analysis for {model_name}:\")\n",
    "    print(\"Train Errors:\")\n",
    "    print(train_errors)\n",
    "    print()\n",
    "\n",
    "    print(\"Validation Errors:\")\n",
    "    print(valid_errors)\n",
    "    print()\n",
    "\n",
    "    print(\"-------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis of the updated CNN model on train and valid dataset\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"cnn_model_updated.h5\")\n",
    "\n",
    "# Get the predicted labels for train and valid datasets\n",
    "train_pred_labels = np.argmax(loaded_model.predict(train_sequences), axis=1)\n",
    "valid_pred_labels = np.argmax(loaded_model.predict(valid_sequences), axis=1)\n",
    "\n",
    "# Get the original labels for train and valid datasets\n",
    "train_original_labels = train_labels\n",
    "valid_original_labels = valid_labels\n",
    "\n",
    "# Create a DataFrame to store the results for train dataset\n",
    "train_error_analysis_df = pd.DataFrame({\"Text\": train_text, \"Original Label\": train_original_labels, \"Predicted Label\": train_pred_labels})\n",
    "\n",
    "# Create a DataFrame to store the results for valid dataset\n",
    "valid_error_analysis_df = pd.DataFrame({\"Text\": valid_text, \"Original Label\": valid_original_labels, \"Predicted Label\": valid_pred_labels})\n",
    "\n",
    "# Filter the DataFrames to get misclassified examples\n",
    "train_misclassified_df = train_error_analysis_df[train_error_analysis_df[\"Original Label\"] != train_error_analysis_df[\"Predicted Label\"]]\n",
    "valid_misclassified_df = valid_error_analysis_df[valid_error_analysis_df[\"Original Label\"] != valid_error_analysis_df[\"Predicted Label\"]]\n",
    "\n",
    "# Print the misclassified examples for train dataset\n",
    "print(\"Misclassified Examples in Train Dataset:\")\n",
    "print(train_misclassified_df)\n",
    "\n",
    "# Print the misclassified examples for valid dataset\n",
    "print(\"Misclassified Examples in Valid Dataset:\")\n",
    "print(valid_misclassified_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469df016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Cross-validation on the updated Linear SVC and SGD Model\n",
    "\n",
    "# Load the train and validation datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "valid_data = pd.read_csv(\"valid.csv\")\n",
    "\n",
    "# Merge train and validation datasets\n",
    "merged_data = pd.concat([train_data, valid_data], ignore_index=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "merged_data = merged_data.dropna()\n",
    "\n",
    "# Extract the text and label columns\n",
    "text = merged_data[\"text\"].tolist()\n",
    "labels = merged_data[\"label\"].tolist()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_text = vectorizer.fit_transform(text)\n",
    "\n",
    "# Load the classifiers from saved .joblib files\n",
    "classifiers = [\n",
    "    (\"Linear SVC\", joblib.load(\"LinearSVC_updated.joblib\")),\n",
    "    (\"SGD Classifier\", joblib.load(\"SGDClassifier_updated.joblib\"))\n",
    "    ]\n",
    "\n",
    "for classifier_name, classifier in classifiers:\n",
    "    print(f\"Results for {classifier_name}:\")\n",
    "    scores = cross_val_score(classifier, vectorized_text, encoded_labels, cv=5, scoring=\"accuracy\")\n",
    "    print(\"Cross-Validation Accuracy:\", scores.mean())\n",
    "    print(\"Cross-Validation Precision:\", cross_val_score(classifier, vectorized_text, encoded_labels, cv=5, scoring=\"precision_weighted\").mean())\n",
    "    print(\"Cross-Validation Recall:\", cross_val_score(classifier, vectorized_text, encoded_labels, cv=5, scoring=\"recall_weighted\").mean())\n",
    "    print(\"Cross-Validation F1 Score:\", cross_val_score(classifier, vectorized_text, encoded_labels, cv=5, scoring=\"f1_weighted\").mean())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cc29bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step\n",
      "Cross-Validation Recall: 0.8052370720077796\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "Cross-Validation F1 Score: 0.80554100341575\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation on the updated CNN model\n",
    "# Load the train and validation datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "valid_data = pd.read_csv(\"valid.csv\")\n",
    "\n",
    "# Merge train and validation datasets\n",
    "merged_data = pd.concat([train_data, valid_data], ignore_index=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "merged_data = merged_data.dropna()\n",
    "\n",
    "# Extract the text and label columns\n",
    "text = merged_data[\"text\"].tolist()\n",
    "labels = merged_data[\"label\"].tolist()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Load the saved CNN model\n",
    "loaded_model = load_model(\"cnn_model_updated.h5\")\n",
    "\n",
    "# Create a function to build the CNN model for KerasClassifier\n",
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model in KerasClassifier\n",
    "keras_classifier = KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Perform cross-validation\n",
    "print(\"Results for CNN model:\")\n",
    "scores = cross_val_score(keras_classifier, sequences, encoded_labels, cv=5, scoring=\"accuracy\")\n",
    "print(\"Cross-Validation Accuracy:\", scores.mean())\n",
    "print(\"Cross-Validation Precision:\", cross_val_score(keras_classifier, sequences, encoded_labels, cv=5, scoring=\"precision_weighted\").mean())\n",
    "print(\"Cross-Validation Recall:\", cross_val_score(keras_classifier, sequences, encoded_labels, cv=5, scoring=\"recall_weighted\").mean())\n",
    "print(\"Cross-Validation F1 Score:\", cross_val_score(keras_classifier, sequences, encoded_labels, cv=5, scoring=\"f1_weighted\").mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432cf83",
   "metadata": {},
   "source": [
    "The Linear SVC model achieved the highest cross-validation accuracy, precision, recall, and F1 score among all the classifiers, showing robust performance on the merged dataset. It performs slightly better than the CNN model in terms of accuracy, precision, and F1 score.\n",
    "\n",
    "The CNN model performs reasonably well and exhibits competitive results compared to other classifiers. However, it falls slightly behind the Linear SVC in terms of accuracy and F1 score, but it is on par with the SGD Classifier.\n",
    "The SGD Classifier also demonstrates good performance, but it falls slightly behind the CNN model and Linear SVC in terms of accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Overall, all three models (CNN, Linear SVC, and SGD Classifier) perform reasonably well on the merged dataset, and there is no significant difference in their performance. Depending on the specific requirements and use case, one might prefer one model over the others. Further fine-tuning and optimization may help to improve the model's performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b11528e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC - Accuracy for test dataset: 0.8002594033722439 F1 Score test dataset: 0.8487229862475442\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation of the Best chosen model - Linear SVC on test dataset\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Check for missing values in the 'text' column and drop rows with NaN\n",
    "test_data = test_data.dropna(subset=['text'])\n",
    "train_data = train_data.dropna(subset=['text'])\n",
    "\n",
    "\n",
    "# Load the saved vectorizer from training\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Transform the test text data using the vectorizer\n",
    "test_vectors = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Load the saved models\n",
    "linear_svc_model = load('LinearSVC_updated.joblib')\n",
    "\n",
    "# Make predictions on the test data using each model\n",
    "linear_svc_pred_test = linear_svc_model.predict(test_vectors)\n",
    "\n",
    "# Calculate accuracy and F1 score for each model on the test data\n",
    "linear_svc_accuracy_test = accuracy_score(test_data['label'], linear_svc_pred_test)\n",
    "linear_svc_f1_test = f1_score(test_data['label'], linear_svc_pred_test, pos_label='QUEER VOICES')\n",
    "\n",
    "# Print the results\n",
    "print(\"Linear SVC - Accuracy for test dataset:\", linear_svc_accuracy_test, \"F1 Score test dataset:\", linear_svc_f1_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc57e2",
   "metadata": {},
   "source": [
    "For the Updated Linear SVC model, the evaluation metrics on the validation set are as follows:\n",
    "\n",
    "Accuracy: 0.6381\n",
    "Precision: 0.6362\n",
    "Recall: 0.9871\n",
    "F1 Score: 0.7737\n",
    "On the other hand, the evaluation metrics on the test set are as follows:\n",
    "\n",
    "Accuracy: 0.8003\n",
    "F1 Score: 0.8487\n",
    "\n",
    "Comparing the results, we can see that the accuracy and F1 score on the test set are higher than those on the validation set. Generally, it is expected that the performance on the validation set should be a good estimate of the model's performance on unseen data, such as the test set. However, there can be some variability due to differences in the data distribution between the two sets.\n",
    "\n",
    "There are several reasons why we might observe such differences between validation and test performance:\n",
    "\n",
    "Data Split: The data split between the validation set and the test set might not be entirely representative of the overall data distribution. There could be variations in the samples or labels between the two sets.\n",
    "\n",
    "Model Overfitting: It is possible that the model is overfitting to the training data, leading to relatively lower performance on the validation set compared to the test set.\n",
    "\n",
    "Randomness: Machine learning models can have some inherent randomness, especially in the case of models like Random Forest and SGDClassifier. These random factors can lead to performance variations on different subsets of data.\n",
    "\n",
    "Class Imbalance: The class distribution in the test set might be different from that in the validation set, especially for the positive class ('QUEER VOICES'), leading to differences in precision and recall.\n",
    "\n",
    "To obtain a more accurate estimate of the model's performance, it is essential to evaluate the model on multiple datasets and consider cross-validation. This helps to mitigate the impact of data splitting and randomness.\n",
    "\n",
    "In summary, while the evaluation metrics on the validation set are a good indicator of model performance, it is normal to observe some variability when applying the model to unseen data (test set). The test set performance should be considered the most reliable measure of the model's generalization capability. If the test set performance is satisfactory, it indicates that the model has learned meaningful patterns and can perform well on new, unseen data. If the test set performance is significantly lower than the validation set, it could indicate potential issues like overfitting or data distribution discrepancies. In such cases, further analysis and tuning may be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da09ac",
   "metadata": {},
   "source": [
    "Validation Metrics for Updated Linear SVC:\n",
    "\n",
    "Accuracy: 0.6381\n",
    "Precision: 0.6362\n",
    "Recall: 0.9871\n",
    "F1 Score: 0.7737\n",
    "Linear SVC for Test Data:\n",
    "\n",
    "Accuracy for the test dataset: 0.8003\n",
    "F1 Score for the test dataset: 0.8487\n",
    "\n",
    "We can see that the performance of the Linear SVC model on the test dataset is noticeably better than on the validation dataset by comparing the validation metrics to the test metrics. Particularly, the test dataset's accuracy and F1 score are higher than those of the validation dataset. This finding implies that the model generalizes well to unknown data, which is indicative of a successful model.\n",
    "\n",
    "The more training data there was (2476 samples in the training set compared to 2476 + 619 = 3095 samples in the combined training and validation set), the better the model did on the test dataset. \n",
    "\n",
    "The model can learn stronger representations of the underlying patterns in the data with more diverse input, which will improve generalization. Additional complex interactions between characteristics and labels can be captured by the model with more data, which may improve performance.\n",
    "\n",
    "It's important to remember, though, that the increase in training data may not be the only factor contributing to the performance improvement. Model performance can also be strongly impacted by additional elements including hyperparameter tweaking, feature selection, and data quality.\n",
    "\n",
    "In conclusion, the results imply that the Linear SVC model has demonstrated greater generalization to the test dataset when trained on additional data, which is a promising result. It emphasizes the significance of having a sizable and varied dataset to train reliable and precise machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2d3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
